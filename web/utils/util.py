import os
import stat
from datetime import datetime

import git
import yaml
import threading
import time
from loguru import logger
from typing import Dict, Callable
from queue import Queue

from openai.openai_object import OpenAIObject

from utils.mysql_connector import MysqlGlobal
from model.tables import project_source
from utils.mysql_utils import mark_project_scanned, add_scan_result


def dump(result):
    logger.info(result.choices[0].message.content)


def dump_file(result, file: str, name: str):
    with open(file, "a+", encoding="utf-8") as f:
        f.write("Scan result of {}\n\n".format(name))
        f.write(result.choices[0].message.content)
        f.write("\n\n------------------------\n\n")


def dump2db(project_id: int, result: OpenAIObject, filename: str):
    add_scan_result(project_id, filename, result.choices[0].message.content, datetime.now())


def dump2(result, name: str):
    logger.info("Scan result of {}\n".format(name))
    dump(result)
    logger.info("\n------------------------\n")


def check_params(**kwargs):
    for k in kwargs:
        if len(kwargs[k]) == 0:
            logger.info("[!] {} is required".format(k))
            exit(0)


def load_conf(**kwargs):
    if len(kwargs["conf"]) == 0:
        return kwargs

    with open(kwargs["conf"], "r", encoding="utf-8") as f:
        conf = yaml.safe_load(f)

    for k in kwargs:
        if type(kwargs[k]) == bool:
            if k in conf:
                kwargs[k] = conf[k]
        elif len(kwargs[k]) == 0:
            if k in conf and len(conf[k]) != 0:
                kwargs[k] = conf[k]
    return kwargs


def clone_repo_from_github(repo: str, project_path: str) -> None:
    """
    clone or pull repository from GitHub.
    """
    logger.info(f'[Clone Repo] start cloning repo: {repo}')
    start_time = time.time()
    need_clone = True
    if os.path.exists(project_path):  # try to execute `git pull` if repo exists
        try:
            git_repo = git.Repo(path=project_path)
            git_repo.remote().pull()
            git_repo.close()
            need_clone = False
        except Exception as e:
            logger.trace(e)
            del_project(project_path=project_path)
    if need_clone:
        try:
            git_repo = git.Repo.clone_from(url=repo, to_path=project_path)
            git_repo.close()
        except Exception as e:
            logger.trace(e)
    end_time = time.time()
    logger.info(f'[Clone Repo] finish cloning repo: {repo}, took time: {end_time - start_time} s')


def del_project(project_path):
    if os.path.exists(project_path):
        for root, dirs, files in os.walk(project_path, topdown=False):
            for name in files:
                os.chmod(os.path.join(root, name), stat.S_IWUSR)
                os.remove(os.path.join(root, name))
            for name in dirs:
                os.chmod(os.path.join(root, name), stat.S_IWUSR)
                os.rmdir(os.path.join(root, name))


def prepare_dir() -> None:
    """
    create the directories for save repos and reports.
    """
    if not os.path.exists("repo"):  # directory `repo` is used to save repository from GitHub.
        os.makedirs("repo")
    if not os.path.exists("results"):  # directory `results` is used to save reports generated by scanner.
        os.makedirs("results")


def generate_repo_params(repo: str) -> Dict[str, str]:
    """
    generate repo_name, project_path(path to save repo), output(path to save report) by repo(Git-Remote url).
    """
    repo_name = repo.split("/")[-1].split(".git")[0]
    return {
        "repo_name": repo_name,
        "project_path": f"./repo/{repo_name}",
        "output": f"results/{repo_name}"
    }


def generate_language_url_dict() -> Dict[str, list]:
    """
    generate dict by programming language and file in corresponding dir. eg. {"c": "https://github.com/t/t.git"}
    """
    lang_url_dict = {}
    if os.path.exists("github_source"):
        for language in os.listdir("github_source"):
            github_url_list = []
            for file_name in os.listdir(f"./github_source/{language}"):
                with open(f"./github_source/{language}/{file_name}", "r", encoding="utf-8") as f:
                    for url in f.readlines():
                        if url.startswith("http"):
                            github_url_list.append(url.replace("\n", "").replace("\r", "").replace("\t", ""))
            lang_url_dict[language] = list(set(github_url_list))
    return lang_url_dict


class MultiThreadingProxy(threading.Thread):
    def __init__(self, thread_name: str, func: Callable[..., None], task_queue: Queue, queue_lock: threading.Lock,
                 exit_after_done: bool, from_db: bool):
        threading.Thread.__init__(self)
        self.name = thread_name
        self.func = func
        self.task_queue = task_queue
        self.queue_lock = queue_lock
        self.exit_after_done = exit_after_done
        self.from_db = from_db

    def run(self):
        logger.info(f"[MultiThreadingProxy] Start thread：{self.name}")
        self.call_func()
        logger.info(f"[MultiThreadingProxy] Start thread：{self.name}")

    def call_func(self):
        while True:
            self.queue_lock.acquire()
            if not self.task_queue.empty():
                task_info = self.task_queue.get()
                url = task_info["url"]
                language = task_info["language"]
                conf = task_info["conf"]
                key = task_info["key"]
                proxy = task_info["proxy"]
                project_id = task_info.get("project_id", None)
                self.queue_lock.release()
                if not isinstance(language, list):
                    language = [language]
                if project_id is None:
                    self.func(repo=url, language=language, include=[], exclude=[], conf=conf, key=key, proxy=proxy)
                else:
                    self.func(project_id=project_id, repo=url, language=language, include=[], exclude=[], conf=conf, key=key, proxy=proxy)
                if self.from_db:
                    mark_project_scanned(url.split("/")[-1].split(".")[0])  # project_name
            else:
                self.queue_lock.release()
                if self.exit_after_done:
                    break
                else:
                    time.sleep(30)
            time.sleep(1)


def start_multi_threading(
        threads: int,
        func: Callable[..., None],
        conf: str,
        key: str,
        proxy: str
) -> None:
    lang_url_dict = generate_language_url_dict()
    queue_lock = threading.Lock()
    thread_list = []
    task_queue = Queue(10000)
    queue_lock.acquire()
    for language in lang_url_dict:
        for url in lang_url_dict[language]:
            task_queue.put({
                "url": url,
                "language": language,
                "conf": conf,
                "key": key,
                "proxy": proxy
            })
    logger.info(f'[Batch GitHub Scan] the length of task_queue: {task_queue.qsize()}')
    queue_lock.release()
    for i in range(threads):
        thread = MultiThreadingProxy(thread_name=f'thread-{i + 1}', func=func, task_queue=task_queue,
                                     queue_lock=queue_lock, exit_after_done=True, from_db=False)
        thread.start()
        thread_list.append(thread)
    while not task_queue.empty():
        pass
    # waiting
    for t in thread_list:
        t.join()


def start_automatic_multi_threading(
        threads: int,
        func: Callable[..., None],
        conf: str,
        key: str,
        proxy: str
) -> None:
    queue_lock = threading.Lock()
    task_queue = Queue(1000000)
    for i in range(threads):
        thread = MultiThreadingProxy(thread_name=f'thread-{i + 1}', func=func, task_queue=task_queue,
                                     queue_lock=queue_lock, exit_after_done=False, from_db=True)
        thread.start()
    project_added = 0
    start_time = time.time()
    while True:
        try:
            if task_queue.qsize() > 30:
                logger.info(f'[Automatic GitHub Scan] The length of task_queue is greater than 30, sleep 60s before next adding project.')
                time.sleep(60)
                continue
            session = MysqlGlobal().mysql_session
            record_list = session.query(project_source).filter(project_source.last_scan == None).limit(threads).all()
            formatted_record_list = [record.get_dict() for record in record_list]
            logger.info(f'[Automatic GitHub Scan] fetched {len(formatted_record_list)} project.')
            if len(formatted_record_list) == 0:
                logger.warning(f'[Automatic GitHub Scan] ATTENTION! There is no not-scanned project. sleep 10min.')
                time.sleep(600)
                continue
            for one_project_source in formatted_record_list:
                logger.info(f'[Automatic GitHub Scan] added {one_project_source["project_name"]} to queue')
                mark_project_scanned(one_project_source["project_name"])
            queue_lock.acquire()
            for one_project_source in formatted_record_list:
                task_queue.put({
                    "project_id": one_project_source["id"],
                    "url": one_project_source["repo_link"],
                    "language": one_project_source["language"],
                    "conf": conf,
                    "key": key,
                    "proxy": proxy
                })
            queue_lock.release()
            queue_len = task_queue.qsize()
            project_scanned = project_added - queue_len
            cost_of_time = time.time() - start_time
            cost_of_hour = int(cost_of_time / 3600)
            cost_of_min = int(cost_of_time / 60)
            cost_of_sec = int(cost_of_time % 60)
            if project_scanned > 0:
                logger.info(
                    f'[Automatic GitHub Scan - Statistics] the length of task_queue: {queue_len}. \n'
                    f'[Automatic GitHub Scan - Statistics] the count of scanned project: {project_scanned}\n'
                    f'[Automatic GitHub Scan - Statistics] the cost of time: {cost_of_hour}:{cost_of_min}:{cost_of_sec}\n'
                    f'[Automatic GitHub Scan - Statistics] the average speed: {cost_of_time/project_scanned} s/per project\n'
                )
            project_added += 10
            time.sleep(30)
        except Exception as e:
            logger.error(f'[Automatic GitHub Scan - ERROR] {e}')
            time.sleep(60)
